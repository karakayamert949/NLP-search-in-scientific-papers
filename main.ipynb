{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2 as pdf\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "FILE_MATCHES = 5\n",
    "SENTENCE_MATCHES = 10\n",
    "\n",
    "\n",
    "def main():\n",
    "    dirName = input(\n",
    "        \"Enter the directory name which papers with txt file extension are located: \")\n",
    "    print(\"Loading Data...\")\n",
    "    files = load_files(dirName)\n",
    "\n",
    "    files_words = {filename: tokenize(files[filename]) for filename in files}\n",
    "\n",
    "    file_idfs = compute_idfs(files_words)\n",
    "    print(\"Data loaded!\")\n",
    "\n",
    "    while True:\n",
    "        query = set(\n",
    "            tokenize(input(\"Enter query:     (enter 'quit' to exit the program)\")))\n",
    "\n",
    "        if len(query) > 1 or str(*query) != \"quit\":\n",
    "            print(\"Please wait procesing...\")\n",
    "            filenames = top_files(query, files_words,\n",
    "                                  file_idfs, n=FILE_MATCHES)\n",
    "\n",
    "            sentences = dict()\n",
    "            for filename in filenames:\n",
    "                for passage in files[filename].split(\"\\n\"):\n",
    "                    for sentence in nltk.sent_tokenize(passage):\n",
    "                        tokens = tokenize(sentence)\n",
    "                        if tokens:\n",
    "                            sentences[sentence] = tokens\n",
    "\n",
    "            idfs = compute_idfs(sentences)\n",
    "\n",
    "            matches = top_sentences(query, sentences, idfs, n=SENTENCE_MATCHES)\n",
    "\n",
    "            print(write_query_result_2txt(filenames, matches))\n",
    "        else:\n",
    "            print(\"You quitted program\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdfs_to_txt(directory):\n",
    "    \"\"\"\n",
    "    Extract texts from pdf files and write their content into new txt files\n",
    "    You only want to do this process once\n",
    "    Inputs a directory name, for every pdf file in the directory\n",
    "    this function creates new .txt files writes the content of pdf\n",
    "    \"\"\"\n",
    "    for file in os.listdir(directory):\n",
    "        pdfFile = open(os.path.join(directory, file), \"rb\")\n",
    "        pdfReader = pdf.PdfFileReader(pdfFile)\n",
    "        numPdfPage = pdfReader.numPages\n",
    "        paperTxt = []\n",
    "        filetxtName = file[:-3]\n",
    "        for pageNum in range(numPdfPage):\n",
    "            paperTxt.append(pdfReader.getPage(pageNum).extractText())\n",
    "        with open(os.path.join(\"papersTXT\", filetxtName), \"w\", encoding=\"utf-8\") as f:\n",
    "            for pagetext in paperTxt:\n",
    "                f.write(pagetext)\n",
    "    return True\n",
    "\n",
    "\n",
    "convert_pdfs_to_txt(\"papersPDF\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(directory):\n",
    "    \"\"\"\n",
    "    Extract text from txt files\n",
    "    Inputs a directory name, \n",
    "    Outputs a dictionary whose keys are names of the files and values are texts of the files\n",
    "    \"\"\"\n",
    "    filesDict = dict()\n",
    "    for file in os.listdir(directory):\n",
    "        with open(os.path.join(directory, file), \"r\", encoding=\"utf-8\") as f:\n",
    "            filesDict[file] = f.read()\n",
    "    return filesDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Process text such that oly alphabetical characters will be left\n",
    "    Convert uppercase characters to lowercase\n",
    "    Inputs a string\n",
    "    Outputs a list of words\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    words = nltk.word_tokenize(text)\n",
    "    tokenizedWords = [word for word in words if word.isalpha()]\n",
    "    return tokenizedWords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idfs(fileDict):\n",
    "    \"\"\"\n",
    "    computes idf values for tokenized words\n",
    "    idf formula:\n",
    "    idf= log(total number of documents/\n",
    "            number of document that contains word)\n",
    "\n",
    "    Inputs a dictionary whose keys are name of the files\n",
    "    and values are tokenized words\n",
    "    Outputs a dictionary whose keys are words and values are idf values\n",
    "    \"\"\"\n",
    "    countDict = dict()\n",
    "    idfDict = dict()\n",
    "    uniqueWords = set(sum(fileDict.values(), []))\n",
    "\n",
    "    for word in uniqueWords:\n",
    "        for content in fileDict.values():\n",
    "            if word in content:\n",
    "                try:\n",
    "                    countDict[word] += 1\n",
    "                except KeyError:\n",
    "                    countDict[word] = 1\n",
    "\n",
    "    numFiles = len(fileDict)\n",
    "    for word, count in countDict.items():\n",
    "        idfDict[word] = np.log(numFiles/count)\n",
    "\n",
    "    return idfDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_files(query, files_words, file_idfs, n):\n",
    "    \"\"\"\n",
    "    Ranks files according to their tf-idf values\n",
    "    Inputs a query (a set of words), files_words (a dictionary mapping\n",
    "    names of files to a list of their words), file_idfs (a dictionary mapping words\n",
    "    to their idf values, n is the number of top n relevant file)\n",
    "    Outputs a list of top \"n\" names of the files\n",
    "    \"\"\"\n",
    "    topDict = dict()\n",
    "    for word in query:\n",
    "        for namefile, content in files_words.items():\n",
    "            tf = content.count(word)\n",
    "            if tf:\n",
    "                try:\n",
    "                    topDict[namefile] += tf*file_idfs[word]\n",
    "                except KeyError:\n",
    "                    topDict[namefile] = tf*file_idfs[word]\n",
    "\n",
    "    sortedFiles = [word for word, val in sorted(\n",
    "        topDict.items(), key=lambda x: x[1], reverse=True)]\n",
    "\n",
    "    return sortedFiles[:n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_sentences(query, sentences, idfs, n):\n",
    "    \"\"\"\n",
    "    Ranks sentences according to their idf values.If there are\n",
    "    ties, preference should be given to sentences that have\n",
    "    a higher query term density.\n",
    "    Inputs a query (a set of words), sentences (a dictionary mapping\n",
    "    sentences to a list of their words), idfs (a dictionary mapping words\n",
    "    to their idf values, n is the number of top n relevant sentences)\n",
    "    Outputs a list of top \"n\" names of the sentences\n",
    "    \"\"\"\n",
    "    topSen = dict()\n",
    "    for sentence, words in sentences.items():\n",
    "        score = 0\n",
    "        for word in query:\n",
    "            if word in words:\n",
    "                try:\n",
    "                    score += idfs[word]\n",
    "                except KeyError:\n",
    "                    score = idfs[word]\n",
    "        if score:\n",
    "            density = words.count(word)/len(words)\n",
    "            topSen[sentence] = (score, density)\n",
    "\n",
    "    sortedSentences = [sentence for sentence, pair in sorted(\n",
    "        topSen.items(), key=lambda x: (x[1][0], x[1][1]), reverse=True)]\n",
    "    return sortedSentences[:n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_query_result_2txt(filenames, matches):\n",
    "    \"\"\"\n",
    "    Print results of the query to a txt file\n",
    "    There are several things you might want to print you can choose whatever you like \n",
    "    \"\"\"\n",
    "    with open(\"Query_result.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"top \"+str(len(matches)) +\n",
    "                \" relevant sentences found in papers\\n\")\n",
    "        f.write(\"top \"+str(len(filenames)) +\n",
    "                \" relevant papers found in collection\\n\\n\")\n",
    "\n",
    "        # f.write top FILE_MATCHES relevant filenames\n",
    "        for filename in filenames:\n",
    "            f.write(filename+\"\\n\")\n",
    "\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        # f.write top SENTENCE_MATCHES relevant sentences\n",
    "        for match in matches:\n",
    "            f.write(match+\"\\n\")\n",
    "\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        # f.write pairs of file name and sentences\n",
    "        for filename, match in zip(filenames, matches):\n",
    "            f.write(f\"{filename}.pdf:   {match}\\n\")\n",
    "    return \"top \"+str(len(matches))+\" relevant sentences found in papers\\n\"\\\n",
    "        + \"top \"+str(len(filenames))+\" relevant papers found in collection\\n\"\\\n",
    "        + \"Result of query is printed to Query_result.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "Data loaded!\n",
      "Please wait procesing...\n",
      "top 10 relevant sentences found in papers\n",
      "top 5 relevant papers found in collection\n",
      "Result of query is printed to Query_result.txt\n",
      "Please wait procesing...\n",
      "top 3 relevant sentences found in papers\n",
      "top 2 relevant papers found in collection\n",
      "Result of query is printed to Query_result.txt\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19f8f9ebccd493d1979261b88c51ecd06bf2efdee26e5f5e5ddb3d1c8ea2e26f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
